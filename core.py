import os
import json
import time
import datetime
import glob
import hashlib
import requests
import html
from dotenv import load_dotenv
import google.generativeai as genai
from pypdf import PdfReader

# Load environment variables
load_dotenv(override=True)

CMS_ROOT = "smart_cms_data"

MAX_INPUT_SIZE = 50000 # Character limit for safety

def check_env_security():
    """Enhanced environment and API Key security verification."""
    if not os.path.exists(".env"):
        return False, "ðŸš¨ SECURITY CRITICAL: .env file missing! Create one based on .env.example"
    
    # Check for placeholder keys
    key = os.getenv("GEMINI_API_KEY")
    if not key or "YOUR_API_KEY" in key or len(key.strip()) < 20:
        return False, "âš ï¸ SECURITY RISK: Invalid or placeholder GEMINI_API_KEY found in .env"
    
    # Check if .env is in .gitignore
    if os.path.exists(".gitignore"):
        with open(".gitignore", "r") as f:
            if ".env" not in f.read():
                return False, "ðŸš¨ SECURITY RISK: .env is not in .gitignore. Do NOT commit your keys!"
        
    return True, "âœ… Security checks passed."

def get_api_key(task_type):
    """
    Retrieves the appropriate API key, falling back to the master key.
    Ensures that placeholder strings are ignored.
    """
    key_map = {
        "creation": "GEMINI_API_KEY_CREATION",
        "transformation": "GEMINI_API_KEY_TRANSFORMATION",
        "cms": "GEMINI_API_KEY_CMS",
        "personalization": "GEMINI_API_KEY_PERSONALIZATION"
    }
    
    master_key = os.getenv("GEMINI_API_KEY")
    env_var = key_map.get(task_type)
    specialized_key = os.getenv(env_var)
    
    # helper for validation
    def is_valid(k):
        return k and "YOUR_NEW_API_KEY" not in k and "PLACEHOLDER" not in k and len(k.strip()) > 30

    if is_valid(specialized_key):
        return specialized_key.strip()
    
    if is_valid(master_key):
        return master_key.strip()
        
    return None

def call_gemini(prompt, task_type, model_name='gemini-1.5-flash'):
    # Input clipping for safety
    prompt = str(prompt)[:MAX_INPUT_SIZE]
    
    api_key = get_api_key(task_type)
    if not api_key:
        return f"Error: API Key for '{task_type}' is missing."
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        # Final sanitization of output
        return sanitize_text(response.text)
    except Exception as e:
        return f"AI Error: {str(e)}"

def generate_hash(content):
    return hashlib.sha256(content.encode('utf-8')).hexdigest()[:12]

def extract_text_from_pdf(file_path):
    try:
        pdf = PdfReader(file_path)
        text = ""
        for i, page in enumerate(pdf.pages):
            if i > 50: # Limit pages for security/performance
                text += "\n[PDF TRUNCATED - Too many pages]"
                break
            text += page.extract_text()
        return sanitize_text(text[:MAX_INPUT_SIZE])
    except Exception as e: return f"Error reading PDF: {e}"

def calculate_reading_time(text):
    words = len(str(text).split())
    minutes = words / 200
    return f"{minutes:.1f} min"

def sanitize_text(text):
    if text is None: return ""
    # More aggressive sanitization could be added here
    # For now, html escaping for display safety
    return html.escape(str(text))


def get_youtube_transcript(url):
    from youtube_transcript_api import YouTubeTranscriptApi
    try:
        video_id = url.split("v=")[1].split("&")[0]
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        return " ".join([t['text'] for t in transcript])
    except Exception as e: return f"Error fetching YouTube transcript: {e}"

def predict_engagement_metrics(content, tone="Professional", platform="Generic", task_type="personalization"):
    """
    AI-powered engagement prediction based on content analysis.
    Returns predicted likes, comments, shares, and engagement score.
    """
    prompt = f"""
    You are an expert social media analyst and engagement predictor.
    
    Analyze the following content and predict realistic engagement metrics:
    
    CONTENT: {content[:3000]}
    TONE: {tone}
    PLATFORM: {platform}
    
    Based on content quality, relevance, tone, and platform best practices, predict:
    1. Expected Likes/Reactions (realistic number)
    2. Expected Comments (realistic number)
    3. Expected Shares (realistic number)
    4. Engagement Score (0-100, where 100 is viral-level engagement)
    5. Best Posting Time (e.g., "Weekday Morning", "Weekend Evening")
    6. Predicted Reach (Low/Medium/High/Viral)
    
    Respond ONLY in this exact JSON format:
    {{
        "likes": <number>,
        "comments": <number>,
        "shares": <number>,
        "engagement_score": <number 0-100>,
        "best_time": "<time recommendation>",
        "predicted_reach": "<Low/Medium/High/Viral>",
        "confidence": <number 0-100>
    }}
    """
    
    try:
        result = call_gemini(prompt, task_type)
        if result and not result.startswith("Error"):
            # Extract JSON from response more robustly
            import re
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        
        # Fallback default predictions
        return {
            "likes": 45,
            "comments": 8,
            "shares": 12,
            "engagement_score": 62,
            "best_time": "Weekday Morning",
            "predicted_reach": "Medium",
            "confidence": 75
        }
    except:
        return {
            "likes": 45,
            "comments": 8,
            "shares": 12,
            "engagement_score": 62,
            "best_time": "Weekday Morning",
            "predicted_reach": "Medium",
            "confidence": 75
        }

def predict_audience_insights(content, audience="General Tech", task_type="personalization"):
    """
    AI-powered audience behavior and preference prediction.
    Returns insights about target audience engagement patterns.
    """
    prompt = f"""
    You are an audience behavior analyst.
    
    Analyze this content and predict audience insights:
    
    CONTENT: {content[:2000]}
    TARGET AUDIENCE: {audience}
    
    Predict:
    1. Primary Age Group (e.g., "18-24", "25-34", "35-44")
    2. Engagement Pattern (e.g., "Quick Scanners", "Deep Readers", "Visual Learners")
    3. Preferred Content Length (Short/Medium/Long)
    4. Key Interest Topics (list 3-5 topics)
    5. Sentiment (Positive/Neutral/Negative)
    
    Respond ONLY in this exact JSON format:
    {{
        "age_group": "<age range>",
        "engagement_pattern": "<pattern>",
        "preferred_length": "<Short/Medium/Long>",
        "interest_topics": ["topic1", "topic2", "topic3"],
        "sentiment": "<Positive/Neutral/Negative>",
        "retention_rate": <number 0-100>
    }}
    """
    
    try:
        result = call_gemini(prompt, task_type)
        if result and not result.startswith("Error"):
            import re
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        
        return {
            "age_group": "25-34",
            "engagement_pattern": "Deep Readers",
            "preferred_length": "Medium",
            "interest_topics": ["Technology", "Innovation", "Productivity"],
            "sentiment": "Positive",
            "retention_rate": 68
        }
    except:
        return {
            "age_group": "25-34",
            "engagement_pattern": "Deep Readers",
            "preferred_length": "Medium",
            "interest_topics": ["Technology", "Innovation", "Productivity"],
            "sentiment": "Positive",
            "retention_rate": 68
        }

def predict_user_behavior(project_history, user_prefs, task_type="personalization"):
    """
    AI-powered predictive modeling of the user's focus and future interaction patterns.
    """
    prompt = f"""
    You are a user behavior predictive model. 
    Analyze the recent interaction history and preferences:
    
    HISTORY: {str(project_history)[:2000]}
    PREFERENCES: {user_prefs}
    
    Predict the following for the next session:
    1. Predicted Intensity (0-100)
    2. Focus Area (e.g. SEO, Tone, Consistency)
    3. Suggested Next Action
    4. User Satisfication Score (0-100 based on past tone feedback)
    
    Respond ONLY in this exact JSON format:
    {{
        "predicted_intensity": <number>,
        "focus_area": "<string>",
        "suggested_action": "<string>",
        "satisfaction_prediction": <number>,
        "learning_confidence": <number>
    }}
    """
    try:
        result = call_gemini(prompt, task_type)
        if result and not result.startswith("Error"):
            import re
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        
        return {
            "predicted_intensity": 75,
            "focus_area": "Content Refinement",
            "suggested_action": "Optimize for Social Reach",
            "satisfaction_prediction": 82,
            "learning_confidence": 65
        }
    except:
        return {
            "predicted_intensity": 75,
            "focus_area": "General Improvement",
            "suggested_action": "Continue Drafting",
            "satisfaction_prediction": 80,
            "learning_confidence": 50
        }

class IngestionClient:
    BASE_URL = "https://ai-enhanced-content-creation-ocr-api.onrender.com/ingest"
    
    def ingest_file(self, file_name, file_content, file_type):
        try:
            files = {'file': (file_name, file_content, file_type)}
            response = requests.post(self.BASE_URL, files=files, timeout=30)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {"error": str(e)}

    def ingest_url(self, url):
        try:
            payload = {"url": url}
            response = requests.post(self.BASE_URL, json=payload, headers={"Content-Type": "application/json"}, timeout=30)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {"error": str(e)}

class ContentManager:
    LIFECYCLE_STAGES = ["Idea", "Draft", "Review", "Approval", "Publication", "Archival"]
    
    def __init__(self):
        if not os.path.exists(CMS_ROOT):
            os.makedirs(CMS_ROOT)
            
    def _get_path(self, folder, project_id):
        return os.path.join(CMS_ROOT, folder, project_id)

    def create_project(self, title, folder, content, tags=None, extra_meta=None):
        timestamp = int(time.time())
        if not title: title = "Untitled Project"
        clean_title = "".join([c if c.isalnum() else "_" for c in title])[:30]
        project_id = f"{timestamp}_{clean_title}"
        path = self._get_path(folder, project_id)
        
        if not os.path.exists(path):
            os.makedirs(path, exist_ok=True)
            
        return self.commit_version(folder, project_id, content, title, tags or [], "Idea", "Initial commit", extra_meta)

    def commit_version(self, folder, project_id, content, title, tags, status, message="Update", extra_meta=None):
        path = self._get_path(folder, project_id)
        if not os.path.exists(path):
            os.makedirs(path, exist_ok=True)
            
        timestamp = datetime.datetime.now().isoformat()
        content_hash = generate_hash(content + timestamp)
        
        word_count = len(content.split())
        char_count = len(content)
        read_time = calculate_reading_time(content)
        
        version_data = {
            "version_id": content_hash,
            "timestamp": timestamp,
            "title": title,
            "content": content,
            "tags": tags,
            "status": status,
            "message": message,
            "metrics": {
                "word_count": word_count,
                "char_count": char_count,
                "read_time": read_time
            },
            "extra_meta": extra_meta or {}
        }
        
        with open(os.path.join(path, f"v_{content_hash}.json"), "w") as f:
            json.dump(version_data, f, indent=2)
            
        meta = {
            "current_head": content_hash,
            "folder": folder,
            "project_id": project_id,
            "last_modified": timestamp,
            "title": title, 
            "tags": tags,
            "status": status,
            "latest_metrics": version_data['metrics']
        }
        with open(os.path.join(path, "meta.json"), "w") as f:
            json.dump(meta, f, indent=2)
            
        return project_id

    def get_meta(self, folder, project_id):
        try:
            with open(os.path.join(self._get_path(folder, project_id), "meta.json"), "r") as f:
                return json.load(f)
        except: return None

    def get_history(self, folder, project_id):
        path = self._get_path(folder, project_id)
        files = glob.glob(os.path.join(path, "v_*.json"))
        history = []
        for f in files:
            with open(f, "r") as r:
                history.append(json.load(r))
        return sorted(history, key=lambda x: x['timestamp'], reverse=True)

    def list_all_content(self):
        projects = []
        for folder_path in glob.glob(os.path.join(CMS_ROOT, "*")):
            if os.path.isdir(folder_path):
                folder_name = os.path.basename(folder_path)
                for proj_path in glob.glob(os.path.join(folder_path, "*")):
                    if os.path.isdir(proj_path):
                        proj_id = os.path.basename(proj_path)
                        meta = self.get_meta(folder_name, proj_id)
                        if meta:
                            projects.append(meta)
        return sorted(projects, key=lambda x: x['last_modified'], reverse=True)
    
    def get_folders(self):
        if not os.path.exists(CMS_ROOT): return []
        return [d for d in os.listdir(CMS_ROOT) if os.path.isdir(os.path.join(CMS_ROOT, d))]
